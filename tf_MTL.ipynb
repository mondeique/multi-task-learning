{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import training/test files in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "train_data = pd.read_csv(os.path.join(os.getcwd(),'training.csv'))\n",
    "test_data = pd.read_csv(os.path.join(os.getcwd(),'test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: we are enabling eager execution for debugging!\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code for handling datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load filenames and labels\n",
    "filenames = tf.constant(train_data.iloc[:, 0].tolist())\n",
    "labels = tf.constant(train_data.iloc[:, 1:].values)\n",
    "\n",
    "# Add to a dataset object\n",
    "dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "\n",
    "# We can debug using eager execution\n",
    "for img, labels in dataset.batch(4).take(1):\n",
    "    print(img)\n",
    "    print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads an image from a file, decodes it into a dense tensor, and resizes it\n",
    "# to a fixed shape.\n",
    "\n",
    "def _parse_function(filename, label):\n",
    "    image_string = tf.read_file(filename) \n",
    "    image_decoded = tf.image.decode_jpeg(image_string, channels=3) # Channels needed because some test images are b/w\n",
    "    image_resized = tf.image.resize_images(image_decoded, [40, 40])\n",
    "    image_shape = tf.cast(tf.shape(image_decoded), tf.float32)\n",
    "    label = tf.concat([label[:]], axis=0)\n",
    "    return {\"x\": image_resized}, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This snippet is adapted from here: https://www.tensorflow.org/guide/datasets\n",
    "def input_fn(dataframe, is_eval=False):\n",
    "\n",
    "    # Load the list of files\n",
    "    filenames = tf.constant(dataframe.iloc[:, 0].tolist())\n",
    "\n",
    "    # Load the labels\n",
    "    labels = tf.constant(dataframe.iloc[:, 1:].values.astype(np.float32))\n",
    "\n",
    "    # Build the dataset with image processing on top of it\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "    dataset = dataset.map(_parse_function)\n",
    "\n",
    "    # Add shuffling and repeatition if training\n",
    "    if is_eval:\n",
    "        dataset = dataset.batch(64)\n",
    "    else:\n",
    "        dataset = dataset.repeat().shuffle(1000).batch(64)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the image & some labels\n",
    "import matplotlib.pyplot as plt\n",
    "for (imgs, labels) in input_fn(train_data, is_eval=True).take(1):\n",
    "    plt.imshow(imgs['x'][0] / 255)\n",
    "    print(labels[0])\n",
    "    print(labels[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard classical estimator (single-task only!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reimplement the feature extraction from the original paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(features):\n",
    "    # Input layer\n",
    "    input_layer = tf.reshape(features[\"x\"], [-1, 40, 40, 3])\n",
    "\n",
    "    # First convolutive layer\n",
    "    conv1 = tf.layers.conv2d(inputs=input_layer, filters=16, kernel_size=[5, 5], padding=\"same\", activation=tf.nn.relu)\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Second convolutive layer\n",
    "    conv2 = tf.layers.conv2d(inputs=pool1, filters=48, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu)\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Third convolutive layer\n",
    "    conv3 = tf.layers.conv2d(inputs=pool2, filters=64, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu)\n",
    "    pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Fourth convolutive layer\n",
    "    conv4 = tf.layers.conv2d(inputs=pool3, filters=64, kernel_size=[2, 2], padding=\"same\", activation=tf.nn.relu)\n",
    "\n",
    "    # Dense Layer\n",
    "    flat = tf.reshape(conv4, [-1, 5 * 5 * 64])\n",
    "    dense = tf.layers.dense(inputs=flat, units=100, activation=tf.nn.relu)\n",
    "  \n",
    "    return dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### single task cnn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from here: https://www.tensorflow.org/tutorials/layers\n",
    "def single_task_cnn_model_fn(features, labels, mode):\n",
    "  \n",
    "    # Get features\n",
    "    dense = extract_features(features)\n",
    "  \n",
    "    # Make predictions\n",
    "    predictions = tf.layers.dense(inputs=dense, units=2)\n",
    "\n",
    "    outputs = {\n",
    "        \"predictions\": predictions\n",
    "    }\n",
    "\n",
    "    # We just want the predictions\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=outputs)\n",
    "\n",
    "    # If not in mode.PREDICT, compute the loss (mean squared error)\n",
    "    loss = tf.losses.mean_squared_error(labels=labels[:], predictions=predictions)\n",
    "\n",
    "    # Single optimization step\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # If not PREDICT or TRAIN, then we are evaluating the model\n",
    "    eval_metric_ops = {\n",
    "        \"rmse\": tf.metrics.root_mean_squared_error(\n",
    "            labels=labels[:, 2:8:5], predictions=outputs[\"predictions\"])}\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_task_classifier = tf.estimator.Estimator(\n",
    "    model_fn=single_task_cnn_model_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & evaluate & test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "single_task_classifier.train(input_fn=lambda: input_fn(train_data), steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_task_classifier.evaluate(input_fn=lambda: input_fn(test_data, is_eval=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = list(single_task_classifier.predict(lambda: input_fn(test_data, is_eval=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate a single prediction\n",
    "for imgs, _ in input_fn(test_data, is_eval=True).take(1):\n",
    "    img_idx = 1\n",
    "    plt.imshow(imgs[\"x\"][img_idx] / 255)\n",
    "    plt.scatter(p[img_idx]['predictions'][0] * 40, p[img_idx]['predictions'][1] * 40, 500, marker='x', color='red', linewidth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplifying the code of the custom estimator with the Head API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the code here: https://www.tensorflow.org/api_docs/python/tf/contrib/estimator/regression_head\n",
    "\n",
    "def single_head_cnn_model_fn(features, labels, mode):\n",
    "\n",
    "    dense = extract_features(features)\n",
    "\n",
    "    # Predictions\n",
    "    predictions = tf.layers.dense(inputs=dense, units=2)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "    # Define the head\n",
    "    regression_head = tf.contrib.estimator.regression_head(label_dimension=2)\n",
    "    return regression_head.create_estimator_spec(features, mode, predictions, labels[:, 2:8:5], optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Estimator\n",
    "cnn_classifier = tf.estimator.Estimator(\n",
    "    model_fn=single_head_cnn_model_fn, model_dir=\"/tmp/cnn_single_head\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-task learning with the Head API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to disable the eager execution at this point\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multihead_input_fn(data, is_eval=False):\n",
    "    features, labels = input_fn(data, is_eval=is_eval).make_one_shot_iterator().get_next() # Why?\n",
    "    return features, {'head_nose': labels[:, 2:8:5], 'head_pose': tf.cast(labels[:, -1] - 1.0, tf.int32)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_cnn_model_fn(features, labels, mode):\n",
    "\n",
    "    dense = extract_features(features)\n",
    "\n",
    "    # Predictions for each task\n",
    "    predictions_nose = tf.layers.dense(inputs=dense, units=2)\n",
    "    predictions_pose = tf.layers.dense(inputs=dense, units=5)\n",
    "    logits = {'head_nose': predictions_nose, 'head_pose': predictions_pose}\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "    # Double head\n",
    "    regression_head = tf.contrib.estimator.regression_head(name='head_nose', label_dimension=2)\n",
    "    classification_head = tf.contrib.estimator.multi_class_head(name='head_pose', n_classes=5)\n",
    "\n",
    "    # Multi head combining two single heads\n",
    "    multi_head = tf.contrib.estimator.multi_head([regression_head, classification_head])\n",
    "\n",
    "    return multi_head.create_estimator_spec(features, mode, logits, labels, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multitask_classifier = tf.estimator.Estimator(model_fn=multi_head_cnn_model_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & evaluate & test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "multitask_classifier.train(input_fn=lambda: multihead_input_fn(train_data), steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multitask_classifier.evaluate(input_fn=lambda: multihead_input_fn(test_data, is_eval=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = list(multitask_classifier.predict(lambda: input_fn(test_data, is_eval=True)))\n",
    "print(p[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
