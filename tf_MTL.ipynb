{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import training/test files in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Desktop/multi-task-learning\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(os.path.join(os.getcwd() + '/data/training_csv'))\n",
    "test_data = pd.read_csv(os.path.join(os.getcwd() + '/data/test_csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mondeique/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ubuntu/anaconda3/envs/mondeique/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ubuntu/anaconda3/envs/mondeique/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ubuntu/anaconda3/envs/mondeique/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ubuntu/anaconda3/envs/mondeique/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ubuntu/anaconda3/envs/mondeique/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/ubuntu/anaconda3/envs/mondeique/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ubuntu/anaconda3/envs/mondeique/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ubuntu/anaconda3/envs/mondeique/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ubuntu/anaconda3/envs/mondeique/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ubuntu/anaconda3/envs/mondeique/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ubuntu/anaconda3/envs/mondeique/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# Note: we are enabling eager execution for debugging!\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'./data/cropped-bag-images-dev/5O2BQILGU6S_1.jpg'\n",
      " b'./data/cropped-bag-images-dev/X63VMOKHOQY_1.jpg'\n",
      " b'./data/cropped-bag-images-dev/TGEWLADRYBR_1.jpg'\n",
      " b'./data/cropped-bag-images-dev/SQJAPKTWWFJ_2.jpg'], shape=(4,), dtype=string)\n",
      "tf.Tensor(\n",
      "[[1]\n",
      " [4]\n",
      " [9]\n",
      " [1]], shape=(4, 1), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# Example code for handling datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load filenames and labels\n",
    "filenames = tf.constant(train_data.iloc[:, 0].tolist())\n",
    "labels = tf.constant(train_data.iloc[:, 1:].values)\n",
    "\n",
    "# Add to a dataset object\n",
    "dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "\n",
    "# We can debug using eager execution\n",
    "for img, labels in dataset.batch(4).take(1):\n",
    "    print(img)\n",
    "    print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads an image from a file, decodes it into a dense tensor, and resizes it\n",
    "# to a fixed shape.\n",
    "\n",
    "def _parse_function(filename, label):\n",
    "    print(filename)\n",
    "    image_string = tf.read_file(filename) \n",
    "    print(image_string)\n",
    "    image_decoded = tf.image.decode_jpeg(image_string, channels=3) # Channels needed because some test images are b/w\n",
    "    image_resized = tf.image.resize_images(image_decoded, [120, 120])\n",
    "    image_shape = tf.cast(tf.shape(image_decoded), tf.float32)\n",
    "#   label = tf.concat([label[:]], axis=0)\n",
    "    return {\"x\": image_resized}, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This snippet is adapted from here: https://www.tensorflow.org/guide/datasets\n",
    "def input_fn(dataframe, is_eval=False):\n",
    "\n",
    "    # Load the list of files\n",
    "    filenames = tf.constant(dataframe.iloc[:, 0].tolist())\n",
    "\n",
    "    # Load the labels\n",
    "    labels = tf.constant(dataframe.iloc[:, 1:].values.astype(np.float32))\n",
    "\n",
    "    # Build the dataset with image processing on top of it\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "    dataset = dataset.map(_parse_function)\n",
    "\n",
    "    # Add shuffling and repeatition if training\n",
    "    if is_eval:\n",
    "        dataset = dataset.batch(64)\n",
    "    else:\n",
    "        dataset = dataset.repeat().shuffle(1000).batch(64)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"args_0:0\", shape=(), dtype=string)\n",
      "Tensor(\"ReadFile:0\", shape=(), dtype=string)\n",
      "tf.Tensor([1.], shape=(1,), dtype=float32)\n",
      "tf.Tensor([4.], shape=(1,), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check the image & some labels\n",
    "import matplotlib.pyplot as plt\n",
    "for (imgs, labels) in input_fn(train_data, is_eval=True).take(1):\n",
    "    plt.imshow(imgs['x'][0] / 255)\n",
    "    print(labels[0])\n",
    "    print(labels[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard classical estimator (single-task only!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reimplement the feature extraction from the original paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(features):\n",
    "    # Input layer\n",
    "    input_layer = tf.reshape(features[\"x\"], [-1, 40, 40, 3])\n",
    "\n",
    "    # First convolutive layer\n",
    "    conv1 = tf.layers.conv2d(inputs=input_layer, filters=16, kernel_size=[5, 5], padding=\"same\", activation=tf.nn.relu)\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Second convolutive layer\n",
    "    conv2 = tf.layers.conv2d(inputs=pool1, filters=48, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu)\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Third convolutive layer\n",
    "    conv3 = tf.layers.conv2d(inputs=pool2, filters=64, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu)\n",
    "    pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Fourth convolutive layer\n",
    "    conv4 = tf.layers.conv2d(inputs=pool3, filters=64, kernel_size=[2, 2], padding=\"same\", activation=tf.nn.relu)\n",
    "\n",
    "    # Dense Layer\n",
    "    flat = tf.reshape(conv4, [-1, 5 * 5 * 64])\n",
    "    dense = tf.layers.dense(inputs=flat, units=100, activation=tf.nn.relu)\n",
    "  \n",
    "    return dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### single task cnn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from here: https://www.tensorflow.org/tutorials/layers\n",
    "def single_task_cnn_model_fn(features, labels, mode):\n",
    "  \n",
    "    # Get features\n",
    "    dense = extract_features(features)\n",
    "  \n",
    "    # Make predictions\n",
    "    predictions = tf.layers.dense(inputs=dense, units=1)\n",
    "\n",
    "    outputs = {\n",
    "        \"predictions\": predictions\n",
    "    }\n",
    "\n",
    "    # We just want the predictions\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=outputs)\n",
    "\n",
    "    # If not in mode.PREDICT, compute the loss (mean squared error)\n",
    "    loss = tf.losses.mean_squared_error(labels=labels[:], predictions=predictions)\n",
    "\n",
    "    # Single optimization step\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # If not PREDICT or TRAIN, then we are evaluating the model\n",
    "    eval_metric_ops = {\n",
    "        \"rmse\": tf.metrics.root_mean_squared_error(\n",
    "            labels=labels[:], predictions=outputs[\"predictions\"])}\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp0wdge__z\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp0wdge__z', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f7ef01faf90>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "single_task_classifier = tf.estimator.Estimator(\n",
    "    model_fn=single_task_cnn_model_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & evaluate & test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/mondeique/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "Tensor(\"args_0:0\", shape=(), dtype=string)\n",
      "Tensor(\"ReadFile:0\", shape=(), dtype=string)\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From <ipython-input-8-bd2814c31cdd>:6: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/mondeique/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7ef023c5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7ef023c5d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7ef023c5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7ef023c5d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:From <ipython-input-8-bd2814c31cdd>:7: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.MaxPooling2D instead.\n",
      "WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7efbfe4710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7efbfe4710>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7efbfe4710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7efbfe4710>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7efbfe4710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7efbfe4710>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7efbfe4710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7efbfe4710>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7efbfe4710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7efbfe4710>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7efbfe4710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7efbfe4710>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7efbfe4710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7efbfe4710>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7efbfe4710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7efbfe4710>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7efbfe4710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7efbfe4710>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7efbfe4710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7efbfe4710>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7efbfe4710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7efbfe4710>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7efbfe4710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7efbfe4710>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:From <ipython-input-8-bd2814c31cdd>:22: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f7efb3a3cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f7efb3a3cd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f7efb3a3cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f7efb3a3cd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f7efb95c550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f7efb95c550>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f7efb95c550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f7efb95c550>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/mondeique/lib/python3.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmp0wdge__z/model.ckpt.\n",
      "INFO:tensorflow:loss = 123.460266, step = 1\n",
      "INFO:tensorflow:global_step/sec: 7.09901\n",
      "INFO:tensorflow:loss = 5.599771, step = 101 (14.087 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.3033\n",
      "INFO:tensorflow:loss = 3.400456, step = 201 (13.692 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.12665\n",
      "INFO:tensorflow:loss = 3.0372272, step = 301 (14.032 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.25993\n",
      "INFO:tensorflow:loss = 1.8506205, step = 401 (13.774 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.29708\n",
      "INFO:tensorflow:loss = 0.53853714, step = 501 (13.704 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.31659\n",
      "INFO:tensorflow:loss = 0.6357757, step = 601 (13.668 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.29566\n",
      "INFO:tensorflow:loss = 0.3076564, step = 701 (13.707 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.41586\n",
      "INFO:tensorflow:loss = 0.21654186, step = 801 (13.485 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.30502\n",
      "INFO:tensorflow:loss = 0.1220272, step = 901 (13.689 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.09774\n",
      "INFO:tensorflow:loss = 0.14277524, step = 1001 (14.089 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.20525\n",
      "INFO:tensorflow:loss = 0.07532454, step = 1101 (13.879 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.00935\n",
      "INFO:tensorflow:loss = 0.07969785, step = 1201 (14.267 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.11859\n",
      "INFO:tensorflow:loss = 0.04285381, step = 1301 (14.047 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.27943\n",
      "INFO:tensorflow:loss = 0.059864856, step = 1401 (13.737 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.40403\n",
      "INFO:tensorflow:loss = 0.052868515, step = 1501 (13.506 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.28514\n",
      "INFO:tensorflow:loss = 0.13672644, step = 1601 (13.727 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.31457\n",
      "INFO:tensorflow:loss = 0.12590608, step = 1701 (13.671 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.24222\n",
      "INFO:tensorflow:loss = 0.104292825, step = 1801 (13.808 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.36856\n",
      "INFO:tensorflow:loss = 0.119227946, step = 1901 (13.571 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.28239\n",
      "INFO:tensorflow:loss = 0.04222888, step = 2001 (13.732 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.27996\n",
      "INFO:tensorflow:loss = 0.031050662, step = 2101 (13.736 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.3574\n",
      "INFO:tensorflow:loss = 0.0517177, step = 2201 (13.592 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.38825\n",
      "INFO:tensorflow:loss = 0.06277114, step = 2301 (13.535 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.32763\n",
      "INFO:tensorflow:loss = 0.03416803, step = 2401 (13.647 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.35797\n",
      "INFO:tensorflow:loss = 0.07974595, step = 2501 (13.590 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.25256\n",
      "INFO:tensorflow:loss = 0.11520254, step = 2601 (13.788 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.30762\n",
      "INFO:tensorflow:loss = 0.17968367, step = 2701 (13.685 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.39168\n",
      "INFO:tensorflow:loss = 0.06873277, step = 2801 (13.529 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.4052\n",
      "INFO:tensorflow:loss = 0.047006585, step = 2901 (13.504 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.17054\n",
      "INFO:tensorflow:loss = 0.039589882, step = 3001 (13.946 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.2927\n",
      "INFO:tensorflow:loss = 0.051588483, step = 3101 (13.712 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.34536\n",
      "INFO:tensorflow:loss = 0.04809293, step = 3201 (13.614 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.28067\n",
      "INFO:tensorflow:loss = 0.11140907, step = 3301 (13.735 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.38122\n",
      "INFO:tensorflow:loss = 0.1359863, step = 3401 (13.548 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.21419\n",
      "INFO:tensorflow:loss = 0.10687327, step = 3501 (13.862 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.23155\n",
      "INFO:tensorflow:loss = 0.068731055, step = 3601 (13.828 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.00699\n",
      "INFO:tensorflow:loss = 0.060206633, step = 3701 (14.271 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.20437\n",
      "INFO:tensorflow:loss = 0.023034386, step = 3801 (13.880 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.22839\n",
      "INFO:tensorflow:loss = 0.020223973, step = 3901 (13.835 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.2281\n",
      "INFO:tensorflow:loss = 0.0623785, step = 4001 (13.835 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.3003\n",
      "INFO:tensorflow:loss = 0.07299223, step = 4101 (13.698 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.19717\n",
      "INFO:tensorflow:loss = 0.14729115, step = 4201 (13.894 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.9731\n",
      "INFO:tensorflow:loss = 0.087651715, step = 4301 (14.341 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4342 into /tmp/tmp0wdge__z/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 7.22065\n",
      "INFO:tensorflow:loss = 0.04808584, step = 4401 (13.849 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.23326\n",
      "INFO:tensorflow:loss = 0.06267898, step = 4501 (13.825 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.31419\n",
      "INFO:tensorflow:loss = 0.054374605, step = 4601 (13.672 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.23861\n",
      "INFO:tensorflow:loss = 0.014288714, step = 4701 (13.815 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.16989\n",
      "INFO:tensorflow:loss = 0.010791378, step = 4801 (13.947 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.31878\n",
      "INFO:tensorflow:loss = 0.03444958, step = 4901 (13.663 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.31333\n",
      "INFO:tensorflow:loss = 0.051628843, step = 5001 (13.674 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.11668\n",
      "INFO:tensorflow:loss = 0.0895609, step = 5101 (14.051 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.1641\n",
      "INFO:tensorflow:loss = 0.074416816, step = 5201 (13.959 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.18774\n",
      "INFO:tensorflow:loss = 0.32310417, step = 5301 (13.913 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.16146\n",
      "INFO:tensorflow:loss = 0.40424725, step = 5401 (13.964 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 7.22328\n",
      "INFO:tensorflow:loss = 0.22462505, step = 5501 (13.844 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.32346\n",
      "INFO:tensorflow:loss = 0.05653099, step = 5601 (13.655 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.14532\n",
      "INFO:tensorflow:loss = 0.036731444, step = 5701 (13.995 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.26094\n",
      "INFO:tensorflow:loss = 0.012891032, step = 5801 (13.772 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.31662\n",
      "INFO:tensorflow:loss = 0.010920709, step = 5901 (13.668 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.33134\n",
      "INFO:tensorflow:loss = 0.009499912, step = 6001 (13.640 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.19284\n",
      "INFO:tensorflow:loss = 0.010677032, step = 6101 (13.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.28356\n",
      "INFO:tensorflow:loss = 0.059183806, step = 6201 (13.729 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.2601\n",
      "INFO:tensorflow:loss = 0.15838788, step = 6301 (13.774 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.33384\n",
      "INFO:tensorflow:loss = 0.226401, step = 6401 (13.635 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.27468\n",
      "INFO:tensorflow:loss = 0.11768245, step = 6501 (13.746 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.32894\n",
      "INFO:tensorflow:loss = 0.04675582, step = 6601 (13.644 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.29894\n",
      "INFO:tensorflow:loss = 0.06689599, step = 6701 (13.701 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.17577\n",
      "INFO:tensorflow:loss = 0.03607009, step = 6801 (13.936 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.31391\n",
      "INFO:tensorflow:loss = 0.015408356, step = 6901 (13.672 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.32069\n",
      "INFO:tensorflow:loss = 0.026040658, step = 7001 (13.660 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.22892\n",
      "INFO:tensorflow:loss = 0.05907969, step = 7101 (13.833 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.36795\n",
      "INFO:tensorflow:loss = 0.042965367, step = 7201 (13.572 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.28178\n",
      "INFO:tensorflow:loss = 0.039769914, step = 7301 (13.733 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.22151\n",
      "INFO:tensorflow:loss = 0.067999765, step = 7401 (13.847 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.37728\n",
      "INFO:tensorflow:loss = 0.10203635, step = 7501 (13.555 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.26587\n",
      "INFO:tensorflow:loss = 0.09947386, step = 7601 (13.763 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.37021\n",
      "INFO:tensorflow:loss = 0.19875205, step = 7701 (13.568 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.22629\n",
      "INFO:tensorflow:loss = 0.13892663, step = 7801 (13.839 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.34884\n",
      "INFO:tensorflow:loss = 0.12796688, step = 7901 (13.607 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.42678\n",
      "INFO:tensorflow:loss = 0.016483111, step = 8001 (13.465 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.27745\n",
      "INFO:tensorflow:loss = 0.013919467, step = 8101 (13.741 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.31341\n",
      "INFO:tensorflow:loss = 0.030095562, step = 8201 (13.673 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.1631\n",
      "INFO:tensorflow:loss = 0.015073907, step = 8301 (13.962 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.26709\n",
      "INFO:tensorflow:loss = 0.018752689, step = 8401 (13.760 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.25784\n",
      "INFO:tensorflow:loss = 0.031944677, step = 8501 (13.778 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.39335\n",
      "INFO:tensorflow:loss = 0.04229159, step = 8601 (13.526 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.16176\n",
      "INFO:tensorflow:loss = 0.037067734, step = 8701 (13.963 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 8703 into /tmp/tmp0wdge__z/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 7.35905\n",
      "INFO:tensorflow:loss = 0.09962313, step = 8801 (13.589 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.28557\n",
      "INFO:tensorflow:loss = 0.38918784, step = 8901 (13.726 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.13576\n",
      "INFO:tensorflow:loss = 0.40549436, step = 9001 (14.014 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.19858\n",
      "INFO:tensorflow:loss = 0.12213646, step = 9101 (13.892 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.28658\n",
      "INFO:tensorflow:loss = 0.024050744, step = 9201 (13.724 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.43636\n",
      "INFO:tensorflow:loss = 0.0083727185, step = 9301 (13.447 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.3527\n",
      "INFO:tensorflow:loss = 0.0029771638, step = 9401 (13.601 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.18856\n",
      "INFO:tensorflow:loss = 0.0022758082, step = 9501 (13.911 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.23865\n",
      "INFO:tensorflow:loss = 0.0007304452, step = 9601 (13.816 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.02296\n",
      "INFO:tensorflow:loss = 0.00084763765, step = 9701 (14.238 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.36223\n",
      "INFO:tensorflow:loss = 0.0015497361, step = 9801 (13.583 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.20817\n",
      "INFO:tensorflow:loss = 0.0045972173, step = 9901 (13.873 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/tmp0wdge__z/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.012724556.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.estimator.Estimator at 0x7f7ef01fa390>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "single_task_classifier.train(input_fn=lambda: input_fn(train_data), steps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"args_0:0\", shape=(), dtype=string)\n",
      "Tensor(\"ReadFile:0\", shape=(), dtype=string)\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7efb8c3c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7efb8c3c90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7efb8c3c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7efb8c3c90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7efb8c3c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7efb8c3c90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7efb8c3c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7efb8c3c90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7efb8c3c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7efb8c3c90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7efb8c3c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7efb8c3c90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7efb8c3c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7efb8c3c90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7efb8c3c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7efb8c3c90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7ea00544d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7ea00544d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7ea00544d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7ea00544d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ea0077c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ea0077c50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ea0077c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ea0077c50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7ea0077c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7ea0077c50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7ea0077c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7ea0077c50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f7efb979c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f7efb979c90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f7efb979c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f7efb979c90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f7efb979c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f7efb979c90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f7efb979c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f7efb979c90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-11-12T19:35:58Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/mondeique/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp0wdge__z/model.ckpt-10000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-11-12-19:35:59\n",
      "INFO:tensorflow:Saving dict for global step 10000: global_step = 10000, loss = 4.411903, rmse = 2.1081843\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 10000: /tmp/tmp0wdge__z/model.ckpt-10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 4.411903, 'rmse': 2.1081843, 'global_step': 10000}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_task_classifier.evaluate(input_fn=lambda: input_fn(test_data, is_eval=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"args_0:0\", shape=(), dtype=string)\n",
      "Tensor(\"ReadFile:0\", shape=(), dtype=string)\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7ea0786290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7ea0786290>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7ea0786290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7ea0786290>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ea0786290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ea0786290>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ea0786290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ea0786290>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7ea0786290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7ea0786290>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7ea0786290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7ea0786290>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ea0786290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ea0786290>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ea0786290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ea0786290>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7ea0786290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7ea0786290>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7ea0786290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7ea0786290>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ea0786290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ea0786290>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ea0786290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ea0786290>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7ea0786290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7ea0786290>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7ea0786290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7ea0786290>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f7ec0138ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f7ec0138ed0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f7ec0138ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f7ec0138ed0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f7ec0138ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f7ec0138ed0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f7ec0138ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f7ec0138ed0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp0wdge__z/model.ckpt-10000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "p = list(single_task_classifier.predict(lambda: input_fn(test_data, is_eval=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'predictions': array([3.2675166], dtype=float32)},\n",
       " {'predictions': array([4.72802], dtype=float32)},\n",
       " {'predictions': array([3.4376438], dtype=float32)},\n",
       " {'predictions': array([7.088169], dtype=float32)},\n",
       " {'predictions': array([4.0717216], dtype=float32)},\n",
       " {'predictions': array([3.0803285], dtype=float32)},\n",
       " {'predictions': array([2.5117178], dtype=float32)},\n",
       " {'predictions': array([5.51766], dtype=float32)},\n",
       " {'predictions': array([4.432986], dtype=float32)},\n",
       " {'predictions': array([2.694408], dtype=float32)},\n",
       " {'predictions': array([6.6536746], dtype=float32)},\n",
       " {'predictions': array([6.3357353], dtype=float32)},\n",
       " {'predictions': array([9.831923], dtype=float32)},\n",
       " {'predictions': array([7.4845657], dtype=float32)},\n",
       " {'predictions': array([5.444915], dtype=float32)},\n",
       " {'predictions': array([5.2777286], dtype=float32)},\n",
       " {'predictions': array([6.1555133], dtype=float32)},\n",
       " {'predictions': array([8.189436], dtype=float32)},\n",
       " {'predictions': array([2.1123526], dtype=float32)},\n",
       " {'predictions': array([5.119011], dtype=float32)},\n",
       " {'predictions': array([5.5807395], dtype=float32)},\n",
       " {'predictions': array([3.4324656], dtype=float32)},\n",
       " {'predictions': array([6.2389393], dtype=float32)},\n",
       " {'predictions': array([4.457837], dtype=float32)},\n",
       " {'predictions': array([1.1122475], dtype=float32)},\n",
       " {'predictions': array([1.5846094], dtype=float32)},\n",
       " {'predictions': array([4.03788], dtype=float32)},\n",
       " {'predictions': array([6.171158], dtype=float32)},\n",
       " {'predictions': array([4.9972687], dtype=float32)},\n",
       " {'predictions': array([7.1298895], dtype=float32)},\n",
       " {'predictions': array([3.740631], dtype=float32)},\n",
       " {'predictions': array([5.4052863], dtype=float32)},\n",
       " {'predictions': array([5.977947], dtype=float32)},\n",
       " {'predictions': array([4.5838437], dtype=float32)},\n",
       " {'predictions': array([6.4834347], dtype=float32)},\n",
       " {'predictions': array([3.5218005], dtype=float32)},\n",
       " {'predictions': array([4.5143375], dtype=float32)},\n",
       " {'predictions': array([4.82261], dtype=float32)},\n",
       " {'predictions': array([4.3180146], dtype=float32)},\n",
       " {'predictions': array([3.5564413], dtype=float32)},\n",
       " {'predictions': array([2.7769017], dtype=float32)},\n",
       " {'predictions': array([5.0775323], dtype=float32)},\n",
       " {'predictions': array([3.6558285], dtype=float32)},\n",
       " {'predictions': array([10.185251], dtype=float32)},\n",
       " {'predictions': array([5.99897], dtype=float32)},\n",
       " {'predictions': array([2.864606], dtype=float32)},\n",
       " {'predictions': array([3.1016812], dtype=float32)},\n",
       " {'predictions': array([7.229871], dtype=float32)},\n",
       " {'predictions': array([3.8237333], dtype=float32)},\n",
       " {'predictions': array([6.3440886], dtype=float32)},\n",
       " {'predictions': array([10.172658], dtype=float32)},\n",
       " {'predictions': array([2.715481], dtype=float32)},\n",
       " {'predictions': array([5.006938], dtype=float32)},\n",
       " {'predictions': array([3.9687648], dtype=float32)},\n",
       " {'predictions': array([3.555809], dtype=float32)},\n",
       " {'predictions': array([3.9060664], dtype=float32)},\n",
       " {'predictions': array([4.158135], dtype=float32)},\n",
       " {'predictions': array([3.7700562], dtype=float32)},\n",
       " {'predictions': array([5.9575624], dtype=float32)},\n",
       " {'predictions': array([5.125843], dtype=float32)},\n",
       " {'predictions': array([9.016062], dtype=float32)},\n",
       " {'predictions': array([7.3069105], dtype=float32)},\n",
       " {'predictions': array([4.450272], dtype=float32)},\n",
       " {'predictions': array([7.7029676], dtype=float32)},\n",
       " {'predictions': array([2.845555], dtype=float32)},\n",
       " {'predictions': array([4.1127977], dtype=float32)},\n",
       " {'predictions': array([5.935504], dtype=float32)},\n",
       " {'predictions': array([4.315995], dtype=float32)},\n",
       " {'predictions': array([4.1409826], dtype=float32)},\n",
       " {'predictions': array([7.024027], dtype=float32)},\n",
       " {'predictions': array([6.6375484], dtype=float32)},\n",
       " {'predictions': array([3.3533545], dtype=float32)},\n",
       " {'predictions': array([4.983617], dtype=float32)},\n",
       " {'predictions': array([4.8523536], dtype=float32)},\n",
       " {'predictions': array([4.6776943], dtype=float32)},\n",
       " {'predictions': array([9.180923], dtype=float32)},\n",
       " {'predictions': array([2.074602], dtype=float32)},\n",
       " {'predictions': array([3.3873565], dtype=float32)},\n",
       " {'predictions': array([7.5888987], dtype=float32)},\n",
       " {'predictions': array([5.605712], dtype=float32)},\n",
       " {'predictions': array([4.2855644], dtype=float32)},\n",
       " {'predictions': array([2.4211185], dtype=float32)},\n",
       " {'predictions': array([5.5311975], dtype=float32)},\n",
       " {'predictions': array([6.25134], dtype=float32)},\n",
       " {'predictions': array([6.687857], dtype=float32)},\n",
       " {'predictions': array([4.6455894], dtype=float32)},\n",
       " {'predictions': array([7.5403504], dtype=float32)},\n",
       " {'predictions': array([7.441073], dtype=float32)},\n",
       " {'predictions': array([9.329085], dtype=float32)},\n",
       " {'predictions': array([5.038231], dtype=float32)},\n",
       " {'predictions': array([6.6522923], dtype=float32)},\n",
       " {'predictions': array([2.4329228], dtype=float32)},\n",
       " {'predictions': array([5.5247707], dtype=float32)},\n",
       " {'predictions': array([8.6322365], dtype=float32)},\n",
       " {'predictions': array([6.8986373], dtype=float32)},\n",
       " {'predictions': array([6.6568274], dtype=float32)},\n",
       " {'predictions': array([4.782504], dtype=float32)},\n",
       " {'predictions': array([6.7379336], dtype=float32)},\n",
       " {'predictions': array([2.7592247], dtype=float32)},\n",
       " {'predictions': array([3.4472048], dtype=float32)},\n",
       " {'predictions': array([7.7952147], dtype=float32)},\n",
       " {'predictions': array([6.146858], dtype=float32)},\n",
       " {'predictions': array([3.6660986], dtype=float32)},\n",
       " {'predictions': array([6.801671], dtype=float32)},\n",
       " {'predictions': array([4.2859964], dtype=float32)},\n",
       " {'predictions': array([5.3220787], dtype=float32)},\n",
       " {'predictions': array([7.8384023], dtype=float32)},\n",
       " {'predictions': array([7.945663], dtype=float32)},\n",
       " {'predictions': array([4.4698267], dtype=float32)},\n",
       " {'predictions': array([3.810164], dtype=float32)},\n",
       " {'predictions': array([4.812416], dtype=float32)},\n",
       " {'predictions': array([4.2214713], dtype=float32)},\n",
       " {'predictions': array([8.172808], dtype=float32)},\n",
       " {'predictions': array([7.3040833], dtype=float32)},\n",
       " {'predictions': array([4.6647387], dtype=float32)},\n",
       " {'predictions': array([4.683422], dtype=float32)},\n",
       " {'predictions': array([3.1936383], dtype=float32)},\n",
       " {'predictions': array([4.525502], dtype=float32)},\n",
       " {'predictions': array([6.5380363], dtype=float32)},\n",
       " {'predictions': array([6.560156], dtype=float32)},\n",
       " {'predictions': array([3.470323], dtype=float32)},\n",
       " {'predictions': array([6.2092004], dtype=float32)},\n",
       " {'predictions': array([3.867982], dtype=float32)},\n",
       " {'predictions': array([2.6740732], dtype=float32)},\n",
       " {'predictions': array([3.684035], dtype=float32)},\n",
       " {'predictions': array([8.785981], dtype=float32)},\n",
       " {'predictions': array([6.31903], dtype=float32)},\n",
       " {'predictions': array([6.255842], dtype=float32)},\n",
       " {'predictions': array([10.411152], dtype=float32)},\n",
       " {'predictions': array([7.3069105], dtype=float32)},\n",
       " {'predictions': array([4.3884034], dtype=float32)},\n",
       " {'predictions': array([2.049064], dtype=float32)},\n",
       " {'predictions': array([3.1181815], dtype=float32)},\n",
       " {'predictions': array([6.1862755], dtype=float32)},\n",
       " {'predictions': array([6.100697], dtype=float32)},\n",
       " {'predictions': array([6.6650004], dtype=float32)},\n",
       " {'predictions': array([4.8652773], dtype=float32)},\n",
       " {'predictions': array([2.2955978], dtype=float32)},\n",
       " {'predictions': array([3.653522], dtype=float32)},\n",
       " {'predictions': array([6.480345], dtype=float32)},\n",
       " {'predictions': array([1.5610586], dtype=float32)},\n",
       " {'predictions': array([6.032588], dtype=float32)},\n",
       " {'predictions': array([6.775634], dtype=float32)},\n",
       " {'predictions': array([3.2355309], dtype=float32)},\n",
       " {'predictions': array([5.104145], dtype=float32)},\n",
       " {'predictions': array([3.8432662], dtype=float32)},\n",
       " {'predictions': array([3.0721917], dtype=float32)},\n",
       " {'predictions': array([7.3069105], dtype=float32)},\n",
       " {'predictions': array([2.8255413], dtype=float32)},\n",
       " {'predictions': array([5.148128], dtype=float32)},\n",
       " {'predictions': array([7.430426], dtype=float32)},\n",
       " {'predictions': array([8.339965], dtype=float32)},\n",
       " {'predictions': array([6.185585], dtype=float32)},\n",
       " {'predictions': array([4.925031], dtype=float32)},\n",
       " {'predictions': array([6.0127997], dtype=float32)},\n",
       " {'predictions': array([4.1150975], dtype=float32)},\n",
       " {'predictions': array([3.5825572], dtype=float32)},\n",
       " {'predictions': array([2.528623], dtype=float32)},\n",
       " {'predictions': array([5.1703544], dtype=float32)},\n",
       " {'predictions': array([3.8450494], dtype=float32)},\n",
       " {'predictions': array([6.9359903], dtype=float32)},\n",
       " {'predictions': array([7.553976], dtype=float32)},\n",
       " {'predictions': array([3.1359396], dtype=float32)},\n",
       " {'predictions': array([5.55492], dtype=float32)},\n",
       " {'predictions': array([5.517617], dtype=float32)},\n",
       " {'predictions': array([4.609692], dtype=float32)},\n",
       " {'predictions': array([6.0113997], dtype=float32)},\n",
       " {'predictions': array([3.7729177], dtype=float32)},\n",
       " {'predictions': array([2.7398288], dtype=float32)},\n",
       " {'predictions': array([2.7393746], dtype=float32)},\n",
       " {'predictions': array([4.590479], dtype=float32)},\n",
       " {'predictions': array([5.600624], dtype=float32)},\n",
       " {'predictions': array([4.9813395], dtype=float32)},\n",
       " {'predictions': array([5.405904], dtype=float32)},\n",
       " {'predictions': array([5.1609316], dtype=float32)},\n",
       " {'predictions': array([5.5567646], dtype=float32)},\n",
       " {'predictions': array([2.6730487], dtype=float32)},\n",
       " {'predictions': array([3.0991976], dtype=float32)},\n",
       " {'predictions': array([6.667453], dtype=float32)},\n",
       " {'predictions': array([2.6315403], dtype=float32)},\n",
       " {'predictions': array([1.9464426], dtype=float32)},\n",
       " {'predictions': array([6.49812], dtype=float32)},\n",
       " {'predictions': array([2.7658637], dtype=float32)},\n",
       " {'predictions': array([3.6774302], dtype=float32)},\n",
       " {'predictions': array([4.1599193], dtype=float32)},\n",
       " {'predictions': array([6.791739], dtype=float32)},\n",
       " {'predictions': array([4.6263947], dtype=float32)},\n",
       " {'predictions': array([3.4988322], dtype=float32)},\n",
       " {'predictions': array([8.540175], dtype=float32)},\n",
       " {'predictions': array([6.32286], dtype=float32)},\n",
       " {'predictions': array([0.5614127], dtype=float32)},\n",
       " {'predictions': array([1.927334], dtype=float32)},\n",
       " {'predictions': array([4.779534], dtype=float32)},\n",
       " {'predictions': array([3.9445934], dtype=float32)},\n",
       " {'predictions': array([4.438965], dtype=float32)},\n",
       " {'predictions': array([1.745205], dtype=float32)},\n",
       " {'predictions': array([2.8783188], dtype=float32)},\n",
       " {'predictions': array([2.7279694], dtype=float32)},\n",
       " {'predictions': array([7.5611925], dtype=float32)},\n",
       " {'predictions': array([5.6605697], dtype=float32)},\n",
       " {'predictions': array([4.14064], dtype=float32)},\n",
       " {'predictions': array([4.5232735], dtype=float32)},\n",
       " {'predictions': array([5.6553664], dtype=float32)},\n",
       " {'predictions': array([6.0517607], dtype=float32)},\n",
       " {'predictions': array([8.937298], dtype=float32)},\n",
       " {'predictions': array([1.5822574], dtype=float32)},\n",
       " {'predictions': array([3.4605439], dtype=float32)},\n",
       " {'predictions': array([7.3639717], dtype=float32)},\n",
       " {'predictions': array([6.181797], dtype=float32)},\n",
       " {'predictions': array([5.9332495], dtype=float32)},\n",
       " {'predictions': array([6.0666656], dtype=float32)},\n",
       " {'predictions': array([5.7319603], dtype=float32)},\n",
       " {'predictions': array([8.847206], dtype=float32)},\n",
       " {'predictions': array([5.7448764], dtype=float32)},\n",
       " {'predictions': array([6.8126955], dtype=float32)},\n",
       " {'predictions': array([3.8532941], dtype=float32)},\n",
       " {'predictions': array([4.0509562], dtype=float32)},\n",
       " {'predictions': array([5.7291894], dtype=float32)},\n",
       " {'predictions': array([2.755082], dtype=float32)},\n",
       " {'predictions': array([2.5261838], dtype=float32)},\n",
       " {'predictions': array([3.792398], dtype=float32)},\n",
       " {'predictions': array([3.6347895], dtype=float32)},\n",
       " {'predictions': array([1.6837097], dtype=float32)},\n",
       " {'predictions': array([7.5333242], dtype=float32)},\n",
       " {'predictions': array([4.146864], dtype=float32)},\n",
       " {'predictions': array([6.297908], dtype=float32)},\n",
       " {'predictions': array([7.2489314], dtype=float32)},\n",
       " {'predictions': array([4.512248], dtype=float32)},\n",
       " {'predictions': array([4.7920604], dtype=float32)},\n",
       " {'predictions': array([4.3631544], dtype=float32)},\n",
       " {'predictions': array([6.2969084], dtype=float32)},\n",
       " {'predictions': array([2.4966717], dtype=float32)},\n",
       " {'predictions': array([0.96295935], dtype=float32)},\n",
       " {'predictions': array([2.4542513], dtype=float32)},\n",
       " {'predictions': array([3.2650387], dtype=float32)},\n",
       " {'predictions': array([5.19662], dtype=float32)},\n",
       " {'predictions': array([4.8598094], dtype=float32)},\n",
       " {'predictions': array([4.5085325], dtype=float32)},\n",
       " {'predictions': array([5.929884], dtype=float32)},\n",
       " {'predictions': array([4.597584], dtype=float32)},\n",
       " {'predictions': array([3.6925151], dtype=float32)},\n",
       " {'predictions': array([3.059684], dtype=float32)},\n",
       " {'predictions': array([0.72247946], dtype=float32)},\n",
       " {'predictions': array([5.8627167], dtype=float32)},\n",
       " {'predictions': array([6.3133087], dtype=float32)},\n",
       " {'predictions': array([4.7239037], dtype=float32)},\n",
       " {'predictions': array([6.812623], dtype=float32)},\n",
       " {'predictions': array([2.8391614], dtype=float32)},\n",
       " {'predictions': array([5.7262278], dtype=float32)},\n",
       " {'predictions': array([5.8790255], dtype=float32)},\n",
       " {'predictions': array([11.182644], dtype=float32)},\n",
       " {'predictions': array([5.857049], dtype=float32)},\n",
       " {'predictions': array([5.3144836], dtype=float32)},\n",
       " {'predictions': array([4.30765], dtype=float32)},\n",
       " {'predictions': array([2.704862], dtype=float32)},\n",
       " {'predictions': array([6.7547545], dtype=float32)},\n",
       " {'predictions': array([3.2745554], dtype=float32)},\n",
       " {'predictions': array([4.5249305], dtype=float32)},\n",
       " {'predictions': array([4.001787], dtype=float32)},\n",
       " {'predictions': array([6.8971033], dtype=float32)},\n",
       " {'predictions': array([5.7679605], dtype=float32)},\n",
       " {'predictions': array([6.327291], dtype=float32)},\n",
       " {'predictions': array([3.6607542], dtype=float32)},\n",
       " {'predictions': array([5.154598], dtype=float32)},\n",
       " {'predictions': array([4.131799], dtype=float32)},\n",
       " {'predictions': array([5.322427], dtype=float32)},\n",
       " {'predictions': array([3.3494928], dtype=float32)},\n",
       " {'predictions': array([7.738216], dtype=float32)},\n",
       " {'predictions': array([8.917914], dtype=float32)},\n",
       " {'predictions': array([3.5924904], dtype=float32)},\n",
       " {'predictions': array([5.594131], dtype=float32)},\n",
       " {'predictions': array([6.491995], dtype=float32)},\n",
       " {'predictions': array([1.5005445], dtype=float32)},\n",
       " {'predictions': array([3.6996677], dtype=float32)},\n",
       " {'predictions': array([6.1689405], dtype=float32)},\n",
       " {'predictions': array([6.914782], dtype=float32)},\n",
       " {'predictions': array([4.871785], dtype=float32)},\n",
       " {'predictions': array([3.743401], dtype=float32)},\n",
       " {'predictions': array([4.202179], dtype=float32)},\n",
       " {'predictions': array([5.9144483], dtype=float32)},\n",
       " {'predictions': array([6.2226214], dtype=float32)},\n",
       " {'predictions': array([4.1416364], dtype=float32)},\n",
       " {'predictions': array([4.3955975], dtype=float32)},\n",
       " {'predictions': array([2.9821134], dtype=float32)},\n",
       " {'predictions': array([5.533793], dtype=float32)},\n",
       " {'predictions': array([3.884705], dtype=float32)},\n",
       " {'predictions': array([5.3478813], dtype=float32)},\n",
       " {'predictions': array([8.131558], dtype=float32)},\n",
       " {'predictions': array([4.9742794], dtype=float32)},\n",
       " {'predictions': array([6.267727], dtype=float32)},\n",
       " {'predictions': array([3.9814172], dtype=float32)},\n",
       " {'predictions': array([3.9775767], dtype=float32)},\n",
       " {'predictions': array([3.8886347], dtype=float32)},\n",
       " {'predictions': array([8.646733], dtype=float32)},\n",
       " {'predictions': array([2.8070724], dtype=float32)},\n",
       " {'predictions': array([7.3042583], dtype=float32)},\n",
       " {'predictions': array([6.0082407], dtype=float32)},\n",
       " {'predictions': array([7.6078005], dtype=float32)},\n",
       " {'predictions': array([5.978576], dtype=float32)},\n",
       " {'predictions': array([5.953681], dtype=float32)},\n",
       " {'predictions': array([6.563334], dtype=float32)},\n",
       " {'predictions': array([6.576778], dtype=float32)},\n",
       " {'predictions': array([3.6160178], dtype=float32)},\n",
       " {'predictions': array([3.279009], dtype=float32)},\n",
       " {'predictions': array([8.388422], dtype=float32)},\n",
       " {'predictions': array([4.9816246], dtype=float32)},\n",
       " {'predictions': array([7.306937], dtype=float32)},\n",
       " {'predictions': array([5.3217335], dtype=float32)},\n",
       " {'predictions': array([2.6231909], dtype=float32)},\n",
       " {'predictions': array([4.936844], dtype=float32)},\n",
       " {'predictions': array([7.7761936], dtype=float32)},\n",
       " {'predictions': array([4.556928], dtype=float32)},\n",
       " {'predictions': array([7.9662924], dtype=float32)},\n",
       " {'predictions': array([8.826216], dtype=float32)},\n",
       " {'predictions': array([7.331898], dtype=float32)},\n",
       " {'predictions': array([3.4108973], dtype=float32)},\n",
       " {'predictions': array([6.4972687], dtype=float32)},\n",
       " {'predictions': array([2.642562], dtype=float32)},\n",
       " {'predictions': array([5.112959], dtype=float32)},\n",
       " {'predictions': array([5.906394], dtype=float32)},\n",
       " {'predictions': array([4.575554], dtype=float32)},\n",
       " {'predictions': array([8.246377], dtype=float32)},\n",
       " {'predictions': array([5.2348547], dtype=float32)},\n",
       " {'predictions': array([6.572322], dtype=float32)},\n",
       " {'predictions': array([1.7989025], dtype=float32)},\n",
       " {'predictions': array([6.012635], dtype=float32)},\n",
       " {'predictions': array([1.329672], dtype=float32)},\n",
       " {'predictions': array([3.750699], dtype=float32)},\n",
       " {'predictions': array([5.7579927], dtype=float32)},\n",
       " {'predictions': array([8.355755], dtype=float32)},\n",
       " {'predictions': array([4.7659903], dtype=float32)},\n",
       " {'predictions': array([2.6149368], dtype=float32)},\n",
       " {'predictions': array([7.3015404], dtype=float32)},\n",
       " {'predictions': array([3.221217], dtype=float32)},\n",
       " {'predictions': array([2.4697623], dtype=float32)},\n",
       " {'predictions': array([7.4694176], dtype=float32)},\n",
       " {'predictions': array([1.6440214], dtype=float32)},\n",
       " {'predictions': array([3.315692], dtype=float32)},\n",
       " {'predictions': array([5.6396046], dtype=float32)},\n",
       " {'predictions': array([4.7483807], dtype=float32)},\n",
       " {'predictions': array([1.8172352], dtype=float32)},\n",
       " {'predictions': array([4.375802], dtype=float32)},\n",
       " {'predictions': array([8.8172455], dtype=float32)},\n",
       " {'predictions': array([5.4456625], dtype=float32)},\n",
       " {'predictions': array([0.6387793], dtype=float32)},\n",
       " {'predictions': array([3.1081815], dtype=float32)},\n",
       " {'predictions': array([4.0713053], dtype=float32)},\n",
       " {'predictions': array([2.382121], dtype=float32)},\n",
       " {'predictions': array([5.548971], dtype=float32)},\n",
       " {'predictions': array([2.0368135], dtype=float32)},\n",
       " {'predictions': array([3.5469964], dtype=float32)},\n",
       " {'predictions': array([4.8252983], dtype=float32)},\n",
       " {'predictions': array([6.089307], dtype=float32)},\n",
       " {'predictions': array([5.9505777], dtype=float32)},\n",
       " {'predictions': array([6.5490685], dtype=float32)},\n",
       " {'predictions': array([5.704403], dtype=float32)},\n",
       " {'predictions': array([7.7046747], dtype=float32)},\n",
       " {'predictions': array([8.549477], dtype=float32)},\n",
       " {'predictions': array([7.466815], dtype=float32)},\n",
       " {'predictions': array([6.3890333], dtype=float32)},\n",
       " {'predictions': array([5.031049], dtype=float32)},\n",
       " {'predictions': array([6.2443085], dtype=float32)},\n",
       " {'predictions': array([6.9108715], dtype=float32)},\n",
       " {'predictions': array([5.56391], dtype=float32)},\n",
       " {'predictions': array([6.7178864], dtype=float32)},\n",
       " {'predictions': array([4.4748025], dtype=float32)},\n",
       " {'predictions': array([4.581834], dtype=float32)},\n",
       " {'predictions': array([4.4400587], dtype=float32)},\n",
       " {'predictions': array([4.332005], dtype=float32)},\n",
       " {'predictions': array([6.6154537], dtype=float32)},\n",
       " {'predictions': array([3.370111], dtype=float32)},\n",
       " {'predictions': array([6.092761], dtype=float32)},\n",
       " {'predictions': array([6.018922], dtype=float32)},\n",
       " {'predictions': array([4.496849], dtype=float32)},\n",
       " {'predictions': array([2.3393476], dtype=float32)},\n",
       " {'predictions': array([7.430992], dtype=float32)},\n",
       " {'predictions': array([7.067278], dtype=float32)},\n",
       " {'predictions': array([6.162286], dtype=float32)},\n",
       " {'predictions': array([4.8956347], dtype=float32)},\n",
       " {'predictions': array([5.858201], dtype=float32)},\n",
       " {'predictions': array([1.768148], dtype=float32)},\n",
       " {'predictions': array([5.704663], dtype=float32)},\n",
       " {'predictions': array([4.309676], dtype=float32)},\n",
       " {'predictions': array([6.036144], dtype=float32)},\n",
       " {'predictions': array([4.0052824], dtype=float32)},\n",
       " {'predictions': array([2.210593], dtype=float32)},\n",
       " {'predictions': array([2.4767988], dtype=float32)},\n",
       " {'predictions': array([1.2644824], dtype=float32)},\n",
       " {'predictions': array([6.487022], dtype=float32)},\n",
       " {'predictions': array([4.3403225], dtype=float32)},\n",
       " {'predictions': array([6.3715906], dtype=float32)},\n",
       " {'predictions': array([11.934203], dtype=float32)},\n",
       " {'predictions': array([8.662038], dtype=float32)},\n",
       " {'predictions': array([7.167863], dtype=float32)},\n",
       " {'predictions': array([3.203699], dtype=float32)},\n",
       " {'predictions': array([7.4337063], dtype=float32)},\n",
       " {'predictions': array([7.7453356], dtype=float32)},\n",
       " {'predictions': array([6.2473583], dtype=float32)},\n",
       " {'predictions': array([5.5628386], dtype=float32)},\n",
       " {'predictions': array([1.9735383], dtype=float32)},\n",
       " {'predictions': array([4.9303656], dtype=float32)},\n",
       " {'predictions': array([5.763505], dtype=float32)},\n",
       " {'predictions': array([6.080116], dtype=float32)},\n",
       " {'predictions': array([6.4836626], dtype=float32)},\n",
       " {'predictions': array([7.625317], dtype=float32)},\n",
       " {'predictions': array([4.897704], dtype=float32)},\n",
       " {'predictions': array([6.1168413], dtype=float32)},\n",
       " {'predictions': array([5.430218], dtype=float32)},\n",
       " {'predictions': array([7.084198], dtype=float32)},\n",
       " {'predictions': array([4.4770117], dtype=float32)},\n",
       " {'predictions': array([6.4521847], dtype=float32)},\n",
       " {'predictions': array([4.5740137], dtype=float32)},\n",
       " {'predictions': array([6.3660583], dtype=float32)},\n",
       " {'predictions': array([3.6624918], dtype=float32)},\n",
       " {'predictions': array([3.8605294], dtype=float32)},\n",
       " {'predictions': array([4.652629], dtype=float32)},\n",
       " {'predictions': array([4.0532513], dtype=float32)},\n",
       " {'predictions': array([5.2370205], dtype=float32)},\n",
       " {'predictions': array([3.7702103], dtype=float32)},\n",
       " {'predictions': array([3.152113], dtype=float32)},\n",
       " {'predictions': array([3.8957481], dtype=float32)},\n",
       " {'predictions': array([5.493542], dtype=float32)},\n",
       " {'predictions': array([6.998805], dtype=float32)},\n",
       " {'predictions': array([3.0683246], dtype=float32)},\n",
       " {'predictions': array([9.816961], dtype=float32)},\n",
       " {'predictions': array([7.1936784], dtype=float32)},\n",
       " {'predictions': array([3.1340828], dtype=float32)},\n",
       " {'predictions': array([5.898085], dtype=float32)},\n",
       " {'predictions': array([1.6476233], dtype=float32)},\n",
       " {'predictions': array([3.282835], dtype=float32)},\n",
       " {'predictions': array([6.3455143], dtype=float32)},\n",
       " {'predictions': array([6.022954], dtype=float32)},\n",
       " {'predictions': array([6.38685], dtype=float32)},\n",
       " {'predictions': array([7.3401546], dtype=float32)},\n",
       " {'predictions': array([9.510731], dtype=float32)},\n",
       " {'predictions': array([5.0106688], dtype=float32)},\n",
       " {'predictions': array([3.5828764], dtype=float32)},\n",
       " {'predictions': array([9.702458], dtype=float32)},\n",
       " {'predictions': array([8.701437], dtype=float32)}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model.ckpt restore and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=g) as sess :\n",
    "\n",
    "    # Saver instance 를 생성한다.\n",
    "    # Saver.restore(sess, ckpt_path)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Saver.restore()\n",
    "    # args : tf.Session, job`s checkpoint file path\n",
    "    # return : None\n",
    "\n",
    "    ckpt_path = saver.restore(sess, \"tmp/~~/model.ckpt-xx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"args_0:0\", shape=(), dtype=string)\n",
      "Tensor(\"ReadFile:0\", shape=(), dtype=string)\n",
      "[3.740631]\n",
      "It is CIRCLE\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate a single prediction\n",
    "for imgs, _ in input_fn(test_data, is_eval=True).take(1):\n",
    "    img_idx = 0\n",
    "    plt.imshow(imgs[\"x\"][img_idx] / 255)\n",
    "    print(p[img_idx]['predictions'])\n",
    "    if 0 < p[img_idx]['predictions'][0] < 1.5:\n",
    "        print(\"It is SQUARE\")\n",
    "    elif 1.5 <= p[img_idx]['predictions'][0] < 2.5:\n",
    "        print(\"It is TRAPEZOID\")\n",
    "    elif 2.5 <= p[img_idx]['predictions'][0] < 3.5:\n",
    "        print(\"It is U_TRAPEZOID\")\n",
    "    elif 3.5 <= p[img_idx]['predictions'][0] < 4.5:\n",
    "        print(\"It is CIRCLE\")\n",
    "    elif 4.5 <= p[img_idx]['predictions'][0] < 5.5:\n",
    "        print(\"It is HALF_CIRCLE\")\n",
    "    elif 5.5 <= p[img_idx]['predictions'][0] < 6.5:\n",
    "        print(\"It is U_HALF_CIRCLE\")\n",
    "    elif 6.5 <= p[img_idx]['predictions'][0] < 7.5:\n",
    "        print(\"It is HOBO\")\n",
    "    elif 7.5 <= p[img_idx]['predictions'][0] < 8.5:\n",
    "        print(\"It is BUCKET\")\n",
    "    elif 8.5 <= p[img_idx]['predictions'][0] < 9.5:\n",
    "        print(\"It is BAKCPACK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-task learning with the Head API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to disable the eager execution at this point\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multihead_input_fn(data, is_eval=False):\n",
    "    features, labels = input_fn(data, is_eval=is_eval).make_one_shot_iterator().get_next() # Why?\n",
    "    return features, {'head_nose': labels[:, 2:8:5], 'head_pose': tf.cast(labels[:, -1] - 1.0, tf.int32)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_cnn_model_fn(features, labels, mode):\n",
    "\n",
    "    dense = extract_features(features)\n",
    "\n",
    "    # Predictions for each task\n",
    "    predictions_nose = tf.layers.dense(inputs=dense, units=2)\n",
    "    predictions_pose = tf.layers.dense(inputs=dense, units=5)\n",
    "    logits = {'head_nose': predictions_nose, 'head_pose': predictions_pose}\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "    # Double head\n",
    "    regression_head = tf.contrib.estimator.regression_head(name='head_nose', label_dimension=2)\n",
    "    classification_head = tf.contrib.estimator.multi_class_head(name='head_pose', n_classes=5)\n",
    "\n",
    "    # Multi head combining two single heads\n",
    "    multi_head = tf.contrib.estimator.multi_head([regression_head, classification_head])\n",
    "\n",
    "    return multi_head.create_estimator_spec(features, mode, logits, labels, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multitask_classifier = tf.estimator.Estimator(model_fn=multi_head_cnn_model_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & evaluate & test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "multitask_classifier.train(input_fn=lambda: multihead_input_fn(train_data), steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multitask_classifier.evaluate(input_fn=lambda: multihead_input_fn(test_data, is_eval=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = list(multitask_classifier.predict(lambda: input_fn(test_data, is_eval=True)))\n",
    "print(p[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
